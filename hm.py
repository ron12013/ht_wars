"""David Donahue 2016. This model will be trained and used to predict winning tweets
in the Semeval 2017 Humor Task. The goal of the model is to read tweet pairs and determine
which of the two tweets in a pair is funnier. For each word in a tweet, it will receive a
phonetic embedding and a GloVe embedding to describe how to pronounce the word and what the
word means, respectively. These will serve as features. Other features may be added over time.
This model will be built in Tensorflow."""

import os
import random
import sys

import numpy as np
import sklearn
import tensorflow as tf

# from sacred import Experiment
# from sacred.observers import MongoObserver

import config
import tools_tf
import tools
import tensorflow.contrib.slim as slim

from keras.layers import Convolution1D, MaxPooling1D
from keras.layers import Input, Dense, Flatten, Embedding
from keras.regularizers import l2


# ex = Experiment('humor_model')
# ex.observers.append(MongoObserver.create(db_name='humor_runs'))


# @ex.config
# def my_config():
#     learning_rate = .00005  # np.random.uniform(.00005, .0000005)
#     num_epochs = 1  # int(np.random.uniform(1.0, 4.0))
#     dropout = 1  # np.random.uniform(.5, 1.0)
#     hidden_dim_size = 800  # int(np.random.uniform(200, 3200))
#     use_emb_model = True
#     use_char_model = True
#     model_save_dir = config.EMB_CHAR_HUMOR_MODEL_DIR
#     if '-emb-only' in sys.argv:
#         use_char_model = False
#         model_save_dir = config.EMB_HUMOR_MODEL_DIR
#     elif '-char-only' in sys.argv:
#         use_emb_model = False
#         model_save_dir = config.CHAR_HUMOR_MODEL_DIR

class HumorModel():
    def __init__(self):
        self.model_vars = None
        self.trainer_vars = None


    def build_trainer(self, model_vars):
        """Take fractional predictions from embedding humor model. Each prediction is a number representing which
        tweet in a pair is funnier(number > 0 => first tweet funnier, number < 0 => second tweet funnier). Runs
        prediction through sigmoid to produce value between 0 and 1. Compares fractional prediction with
        actual label using tf.sigmoid_cross_entropy_with_logits() (sigmoid is done internally)."""
        print 'Building embedding humor model trainer'
        tf_labels = tf.placeholder(dtype=tf.int32, shape=[None], name='labels')
        [tf_first_input_tweets, tf_second_input_tweets, output, tf_tweet_humor_rating, tf_batch_size, tf_hashtag,
         output_prob, tf_dropout_rate, tf_tweet1, tf_tweet2] = model_vars
        tf_reshaped_labels = tf.cast(tf.reshape(tf_labels, [-1, 1]), tf.float32)

        tf_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=tf_tweet_humor_rating, labels=tf_reshaped_labels)

        tf_loss = tf.reduce_sum(tf_cross_entropy) / tf.cast(tf_batch_size, tf.float32)

        slim.losses.add_loss(tf_loss)

        loss_total = slim.losses.get_total_loss(add_regularization_losses=False)

        return [loss_total, tf_labels]

    def build(self, vocab_size, use_embedding_model=True, use_character_model=True, hidden_dim_size=None):
        """Takes in two tweets. For each word in each tweet, if use_embedding_model is true, the model is given a GloVe embedding and a
        phonetic embedding(generated by phoneme_model). If use_character_model is true, it
        will construct a convolutional character-based model. It groups the output of both these models by concatenation,
        and then makes a prediction of which tweet is funnier using three fully-connected layers."""
        print 'Building embedding humor model'

        tf_batch_size, tf_dropout_rate, tf_first_input_tweets, \
        tf_first_tweet_encoder_output, tf_hashtag, \
        tf_second_input_tweets, tf_second_tweet_encoder_output = self.build_embedding_config(lstm_hidden_dim=hidden_dim_size)

        # Create character model to feed into dense layers
        tweet1_conv_emb, tweet2_conv_emb, tf_tweet1, tf_tweet2 = self.build_character_config(config.TWEET_SIZE, vocab_size)

        # Add pop culture features here

        # Concatenate encoders and process with dense layers
        dense_features = []
        if use_embedding_model:
            dense_features.append(tf_first_tweet_encoder_output)
            dense_features.append(tf_second_tweet_encoder_output)
        if use_character_model:
            dense_features.append(tweet1_conv_emb)
            dense_features.append(tweet2_conv_emb)
        tf_tweet_pair_emb = tf.concat(dense_features, 1)
        print tf_tweet_pair_emb.get_shape()
        tweet_pair_emb_size = int(tf_tweet_pair_emb.get_shape()[1])
        tf_tweet_pair_emb_dropout = tf.nn.dropout(tf_tweet_pair_emb, keep_prob=tf_dropout_rate)

        tf_tweet_dense_layer1, _, _ = tools_tf.create_dense_layer(tf_tweet_pair_emb_dropout, tweet_pair_emb_size,
                                                         tweet_pair_emb_size * 3 / 4, activation='relu', name='layer_1')
        tf_tweet_dense_layer2, _, _ = tools_tf.create_dense_layer(tf_tweet_dense_layer1, tweet_pair_emb_size * 3 / 4,
                                                         tweet_pair_emb_size / 2, activation='relu', name='layer_2')
        tf_tweet_humor_rating, _, _ = tools_tf.create_dense_layer(tf_tweet_dense_layer2, tweet_pair_emb_size / 2, 1,
                                                         name='layer_3')

        # Code from Alexey to transform fractional predictions into prediction labels
        output_logits = tf.reshape(tf_tweet_humor_rating, [-1])
        output_prob = tf.nn.sigmoid(output_logits)
        output = tf.where(tf.greater_equal(output_prob, 0.5),
                          x=tf.ones_like(output_prob, dtype=tf.int32),
                          y=tf.zeros_like(output_prob, dtype=tf.int32))

        # Print model variables (debug)
        trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)
        for var in trainable_vars:
            print var.name + ' ' + str(var.get_shape())

        return [tf_first_input_tweets, tf_second_input_tweets, output, tf_tweet_humor_rating, tf_batch_size,
                tf_hashtag, output_prob, tf_dropout_rate, tf_tweet1, tf_tweet2]  # Model vars
        #
        # return {'first_input_tweets':tf_first_input_tweets, 'second_input_tweets':tf_second_input_tweets,
        #         'output':output, 'humor_rating':tf_tweet_humor_rating, 'batch_size':tf_batch_size
        #         'output_prob':output_prob, 'keep_rate':tf_dropout_rate, 'first_input_tweets_char':tf_tweet1,
        #         'second_input_tweets_char': tf_tweet2}  # Model vars

    def build_character_config(self, tweet_size, vocab_size):
        """Load two tweets, analyze them with convolution and predict which is funnier."""
        print 'Building model'
        # Model parameters I can mess with:
        num_filters_1 = 32
        num_filters_2 = 64
        filter_size_1 = 3
        filter_size_2 = 5
        dropout = 0.7
        fc1_dim = 200
        fc2_dim = 50
        tweet_emb_dim = 50
        pool_length = 5

        print 'Vocabulary size: %s' % vocab_size
        # Two tweets as input. Run them through an embedding layer
        tweet1 = Input(shape=[tweet_size])
        tweet2 = Input(shape=[tweet_size])

        tweet_input_emb_lookup = Embedding(vocab_size, tweet_emb_dim, input_length=tweet_size)
        tweet1_emb = tweet_input_emb_lookup(tweet1)
        tweet2_emb = tweet_input_emb_lookup(tweet2)

        # Run both tweets separately through convolution layers, a max pool layer,
        # and then flatten them for dense layer.
        convolution_layer_1 = Convolution1D(num_filters_1, filter_size_1, input_shape=[tweet_size, vocab_size])
        convolution_layer_2 = Convolution1D(num_filters_2, filter_size_2)
        max_pool_layer = MaxPooling1D(strides=4)
        flatten = Flatten()
        tweet_conv_emb = Dense(fc1_dim, activation='relu')

        tweet_1_conv1 = max_pool_layer(convolution_layer_1(tweet1_emb))
        tweet_2_conv1 = max_pool_layer(convolution_layer_1(tweet2_emb))
        tweet1_conv2 = flatten(max_pool_layer(convolution_layer_2(tweet_1_conv1)))
        tweet2_conv2 = flatten(max_pool_layer(convolution_layer_2(tweet_2_conv1)))
        tweet1_conv_emb = tweet_conv_emb(tweet1_conv2)
        tweet2_conv_emb = tweet_conv_emb(tweet2_conv2)

        return tweet1_conv_emb, tweet2_conv_emb, tweet1, tweet2

    def build_embedding_config(self, lstm_hidden_dim=None):
        """Applies dropout to and feeds two tweets in separate tweet encoders(shared weights)."""
        # Create placeholders
        tf_batch_size = tf.placeholder(tf.int32, name='batch_size')
        tf_dropout_rate = tf.placeholder(tf.float32, name='dropout_rate')
        word_embedding_size = config.GLOVE_EMB_SIZE + config.PHONETIC_EMB_SIZE
        if lstm_hidden_dim is None:
            lstm_hidden_dim = word_embedding_size * 2
        tf_first_input_tweets = tf.placeholder(dtype=tf.float32,
                                               shape=[None, config.HUMOR_MAX_WORDS_IN_TWEET * word_embedding_size],
                                               name='first_tweets')
        tf_second_input_tweets = tf.placeholder(dtype=tf.float32,
                                                shape=[None, config.HUMOR_MAX_WORDS_IN_TWEET * word_embedding_size],
                                                name='second_tweets')
        tf_hashtag = tf.placeholder(dtype=tf.float32,
                                    shape=[None, config.HUMOR_MAX_WORDS_IN_HASHTAG * word_embedding_size],
                                    name='hashtag')
        # Create tweet LSTM encoders to feed into dense layers
        tf_first_input_tweets_dropout = tf.nn.dropout(tf_first_input_tweets, tf_dropout_rate)
        tf_second_input_tweets_dropout = tf.nn.dropout(tf_second_input_tweets, tf_dropout_rate)

        tf_first_tweet_encoder_output, tf_first_tweet_hidden_state = tools_tf.build_lstm(lstm_hidden_dim, tf_batch_size,
                                                                                [tf_first_input_tweets_dropout],
                                                                                word_embedding_size,
                                                                                config.HUMOR_MAX_WORDS_IN_TWEET,
                                                                                lstm_scope='TWEET_ENCODER',
                                                                                time_step_inputs=[])
        tf_second_tweet_encoder_output, tf_second_tweet_hidden_state = tools_tf.build_lstm(lstm_hidden_dim, tf_batch_size,
                                                                                  [tf_second_input_tweets_dropout],
                                                                                  word_embedding_size,
                                                                                  config.HUMOR_MAX_WORDS_IN_TWEET,
                                                                                  lstm_scope='TWEET_ENCODER',
                                                                                  reuse=True, time_step_inputs=[])
        return tf_batch_size, tf_dropout_rate, tf_first_input_tweets, tf_first_tweet_encoder_output, tf_hashtag, tf_second_input_tweets, tf_second_tweet_encoder_output


def train_on_all_other_hashtags(model_vars, trainer_vars, hashtag_names, hashtag_datas, n_epochs=1,
                                batch_size=50, learning_rate=5e-4,
                                dropout=config.HUMOR_KEEP_PROB, model_save_dir=config.EMB_CHAR_HUMOR_MODEL_DIR, leave_out_hashtags=None):
    """Trains on all hashtags in the SEMEVAL_HUMOR_TRAIN_DIR directory. Extracts inputs and labels from
    each hashtag, and trains in batches. Inserts input into model and evaluates output using model_vars.
    Minimizes loss defined in trainer_vars. Repeats for n_epoch epochs. For zero epochs, the model is not trained
    and an accuracy of -1 is returned."""
    if leave_out_hashtags is None:
        leave_out_hashtags = []
    if isinstance(leave_out_hashtags, str):
        leave_out_hashtags = [leave_out_hashtags]
    if not os.path.exists(model_save_dir):
        os.makedirs(model_save_dir)
    [tf_first_input_tweets, tf_second_input_tweets, tf_predictions, tf_tweet_humor_rating, tf_batch_size, tf_hashtag,
     output_prob, tf_dropout_rate,
     tf_tweet1, tf_tweet2] = model_vars
    [tf_loss, tf_labels] = trainer_vars

    sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=tools_tf.GPU_OPTIONS))
    train_op = tf.train.AdamOptimizer(learning_rate).minimize(tf_loss)
    init = tf.global_variables_initializer()
    with tf.name_scope("SAVER"):
        saver = tf.train.Saver(max_to_keep=10)
    sess.run(init)

    accuracies = []
    for epoch in range(n_epochs):
        if n_epochs > 1:
            print 'Epoch %s' % epoch
        print hashtag_names
        for trainer_hashtag_name in hashtag_names:
            print trainer_hashtag_name
            if trainer_hashtag_name not in leave_out_hashtags:
                # Train on this hashtag.
                np_first_tweets, np_second_tweets, np_labels, first_tweet_ids, second_tweet_ids, np_hashtag = \
                    tools.load_hashtag_data(config.HUMOR_TRAIN_TWEET_PAIR_EMBEDDING_DIR, trainer_hashtag_name)

                np_first_tweets_char, np_second_tweets_char = \
                    tools.extract_tweet_pair_from_hashtag_datas(hashtag_datas, trainer_hashtag_name)
                current_batch_size = batch_size
                # Use these parameters to keep track of batch size and start location.
                starting_training_example = 0
                num_batches = np_first_tweets.shape[0] / batch_size
                remaining_batch_size = np_first_tweets.shape[0] % batch_size
                batch_accuracies = []
                batch_losses = []
                # print 'Training on hashtag %s' % trainer_hashtag_name
                for i in range(num_batches + 1):
                    # If we are on the last batch, we are running leftover examples.
                    # Otherwise, we stick to global batch_size.
                    if i == num_batches:
                        current_batch_size = remaining_batch_size
                    else:
                        current_batch_size = batch_size
                    if current_batch_size > 0:
                        np_batch_first_tweets = np_first_tweets[
                                                starting_training_example:starting_training_example + current_batch_size,
                                                :]
                        np_batch_second_tweets = np_second_tweets[
                                                 starting_training_example:starting_training_example + current_batch_size,
                                                 :]
                        np_batch_labels = np_labels[
                                          starting_training_example:starting_training_example + current_batch_size]
                        np_batch_hashtag = np_hashtag[
                                           starting_training_example:starting_training_example + current_batch_size, :]
                        np_batch_first_tweets_char = np_first_tweets_char[
                                                     starting_training_example:starting_training_example + current_batch_size,
                                                     :]
                        np_batch_second_tweets_char = np_second_tweets_char[
                                                      starting_training_example:starting_training_example + current_batch_size,
                                                      :]

                        [np_batch_predictions, batch_loss, _] = sess.run([tf_predictions, tf_loss, train_op],
                                                                         feed_dict={
                                                                             tf_first_input_tweets: np_batch_first_tweets,
                                                                             tf_second_input_tweets: np_batch_second_tweets,
                                                                             tf_labels: np_batch_labels,
                                                                             tf_batch_size: current_batch_size,
                                                                             tf_hashtag: np_batch_hashtag,
                                                                             tf_dropout_rate: dropout,
                                                                             tf_tweet1: np_batch_first_tweets_char,
                                                                             tf_tweet2: np_batch_second_tweets_char})

                        batch_accuracy = sklearn.metrics.accuracy_score(np_batch_labels, np_batch_predictions)

                        batch_accuracies.append(batch_accuracy)
                        batch_losses.append(batch_loss)

                    starting_training_example += current_batch_size

                hashtag_accuracy = np.mean(batch_accuracies)
                hashtag_loss = np.mean(batch_losses)
                print 'Hashtag %s accuracy: %s' % (trainer_hashtag_name, hashtag_accuracy)
                print 'Hashtag loss: %s' % hashtag_loss
                accuracies.append(hashtag_accuracy)

            else:
                print 'Do not train on current hashtag: %s' % trainer_hashtag_name
        print 'Saving..'
        saver.save(sess, os.path.join(model_save_dir, 'emb_humor_model'),
                   global_step=epoch)  # Save model after every epoch
    if len(accuracies) > 0:
        training_accuracy = np.mean(accuracies)
    else:
        training_accuracy = -1

    # Save trained model.

    return sess, training_accuracy


def calculate_accuracy_on_batches(batch_predictions, np_labels):
    """batch_predictions is a list of numpy arrays. Each numpy
    array represents the predictions for a single batch. Evaluates
    performance of each batch using np_labels. There must be
    at least one batch containing at least one example"""
    accuracy_sum = 0
    total_examples = 0
    for np_batch_predictions in batch_predictions:
        batch_accuracy = np.mean(np_batch_predictions == np_labels)
        batch_size = np_batch_predictions.shape[0]
        if batch_size > 0:
            accuracy_sum += batch_accuracy * batch_size
            total_examples += batch_size
    return accuracy_sum / total_examples


def load_build_train_and_predict(learning_rate, num_epochs, dropout, use_emb_model,
                                 use_char_model, model_save_dir, hidden_dim_size, leave_out_hashtags=[]):
    """Builds and trains a humor model on the semeval task training set. Evaluates on the semeval task trial set,
    prints accuracy. Saves model after each epoch of training.

    learning_rate - rate at which model learns dataset
    num_epochs - number of times model trains on entire training dataset
    dropout - specifies the keep rate of dropout in the model. 1 means no dropout
    use_emb_model - use embedding features in model prediction
    use_char_model - use characters of tweet as features in model prediction
    model_save_dir - where to save model parameters after each epoch
    hidden_dim_size - size of lstm embedding encoder
    leave_out_hashtags - hashtag names to omit from training step (could use for ensemble model training)"""
    print 'Learning rate: %s' % learning_rate
    print 'Number of epochs: %s' % num_epochs
    print 'Dropout keep rate: %s' % dropout
    print 'Use embedding model: %s' % use_emb_model
    print 'Use character model: %s' % use_char_model
    print 'Model save directory: %s' % model_save_dir
    print 'Tweet encoder state size: %s' % hidden_dim_size

    random.seed('hello world')
    hashtag_datas, char_to_index, vocab_size = tools.load_hashtag_data_and_vocabulary(config.HUMOR_TRAIN_TWEET_PAIR_CHAR_DIR,
                                                                                config.HUMOR_CHAR_TO_INDEX_FILE_PATH)
    trial_hashtag_datas, _, trial_vocab_size = \
        tools.load_hashtag_data_and_vocabulary(config.HUMOR_TRIAL_TWEET_PAIR_CHAR_DIR, None)

    hm = HumorModel()

    g = tf.Graph()
    with g.as_default():
        model_vars = hm.build(vocab_size, use_embedding_model=use_emb_model,
                              use_character_model=use_char_model,
                              hidden_dim_size=hidden_dim_size)
        trainer_vars = hm.build_trainer(model_vars)
        tools_tf.create_tensorboard_visualization('emb_humor_model')
        training_hashtag_names = tools.get_hashtag_file_names(config.SEMEVAL_HUMOR_TRAIN_DIR)
        testing_hashtag_names = tools.get_hashtag_file_names(config.SEMEVAL_HUMOR_TRIAL_DIR)
        accuracies = []

        sess, training_accuracy = train_on_all_other_hashtags(model_vars, trainer_vars, training_hashtag_names,
                                                              hashtag_datas, n_epochs=num_epochs,
                                                              learning_rate=learning_rate,
                                                              dropout=dropout,
                                                              model_save_dir=model_save_dir,
                                                              leave_out_hashtags=leave_out_hashtags)
        print 'Mean training accuracy: %s' % training_accuracy
        print
        for hashtag_name in testing_hashtag_names:
            accuracy, _ = tools_tf.predict_on_hashtag(sess,
                                                      model_vars,
                                                      hashtag_name,
                                                      config.HUMOR_TRIAL_TWEET_PAIR_EMBEDDING_DIR,
                                                      trial_hashtag_datas,
                                                      error_analysis_stats=[config.SEMEVAL_HUMOR_TRIAL_DIR, 10])
            print 'Hashtag %s accuracy: %s' % (hashtag_name, accuracy)
            accuracies.append(accuracy)

        test_accuracy = np.mean(accuracies)
        print 'Final test accuracy: %s' % test_accuracy

    return {'train_accuracy': training_accuracy,
            'test_accuracy': test_accuracy}


#@ex.main
def main(learning_rate, num_epochs, dropout, use_emb_model,
         use_char_model, model_save_dir, hidden_dim_size, leave_out_hashtags=[]):
    load_build_train_and_predict(learning_rate, num_epochs, dropout, use_emb_model,
                                 use_char_model, model_save_dir, hidden_dim_size, leave_out_hashtags=[])


if __name__ == '__main__':
    # num_experiments_run = 1
    # for index in range(num_experiments_run):
    #     print 'Experiment: %s' % index
    #     r = ex.run()
    learning_rate = .00005  # np.random.uniform(.00005, .0000005)
    num_epochs = 1  # int(np.random.uniform(1.0, 4.0))
    dropout = 1  # np.random.uniform(.5, 1.0)
    hidden_dim_size = 800  # int(np.random.uniform(200, 3200))
    use_emb_model = True
    use_char_model = True
    model_save_dir = config.EMB_CHAR_HUMOR_MODEL_DIR
    if '-emb-only' in sys.argv:
        use_char_model = False
        model_save_dir = config.EMB_HUMOR_MODEL_DIR
    elif '-char-only' in sys.argv:
        use_emb_model = False
        model_save_dir = config.CHAR_HUMOR_MODEL_DIR
    load_build_train_and_predict(learning_rate, num_epochs, dropout, use_emb_model,
                             use_char_model, model_save_dir, hidden_dim_size)
