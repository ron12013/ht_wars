"""David Donahue 2016. This model will be trained and used to predict winning tweets
in the Semeval 2017 Humor Task. The goal of the model is to read tweet pairs and determine
which of the two tweets in a pair is funnier. For each word in a tweet, it will receive a
phonetic embedding and a GloVe embedding to describe how to pronounce the word and what the
word means, respectively. These will serve as features. Other features may be added over time.
This model will be built in Tensorflow."""
import tensorflow as tf
import numpy as np
import cPickle as pickle
import random
import os
import sklearn
from config import HUMOR_TRAIN_TWEET_PAIR_EMBEDDING_DIR
from config import HUMOR_TRIAL_TWEET_PAIR_EMBEDDING_DIR
from config import EMBEDDING_HUMOR_MODEL_DIR
from config import SEMEVAL_HUMOR_TRIAL_DIR
from tools import HUMOR_MAX_WORDS_IN_TWEET
from tools import HUMOR_MAX_WORDS_IN_HASHTAG
from config import SEMEVAL_HUMOR_TRAIN_DIR
from config import SEMEVAL_HUMOR_TRIAL_DIR
from config import HUMOR_CHAR_TO_INDEX_FILE_PATH
from config import HUMOR_TWEET_PAIR_DIR
from tools import GLOVE_SIZE
from tools import PHONETIC_EMB_SIZE
from tools import get_hashtag_file_names
from tools import load_hashtag_data
from tools import load_hashtag_data_and_vocabulary
from tf_tools import GPU_OPTIONS
from tf_tools import create_dense_layer
from tf_tools import predict_on_hashtag
from tf_tools import create_tensorboard_visualization
from keras.models import Model
from keras.layers import Input, Dense, Dropout, Flatten, merge, Embedding
from keras.layers import Convolution1D, MaxPooling1D


EMBEDDING_HUMOR_MODEL_LEARNING_RATE = .000005
N_TRAIN_EPOCHS = 1
DROPOUT = 1  # Off
TWEET_SIZE = 140


def main():
    """Game plan: Use train_dir hashtags to train the model. Then use trial_dir hashtags
    to evaluate its performance. This creates a faster development environment than
    leave-one-out."""
    # hashtag_datas, char_to_index, vocab_size = load_hashtag_data_and_vocabulary(HUMOR_TWEET_PAIR_DIR, HUMOR_CHAR_TO_INDEX_FILE_PATH)

    model_vars = build_embedding_humor_model()
    trainer_vars = build_embedding_humor_model_trainer(model_vars)
    create_tensorboard_visualization('emb_humor_model')
    training_hashtag_names = get_hashtag_file_names(SEMEVAL_HUMOR_TRAIN_DIR)
    testing_hashtag_names = get_hashtag_file_names(SEMEVAL_HUMOR_TRIAL_DIR)
    accuracies = []

    # Blank hashtag '' means train on all training hashtags
    sess, training_accuracy = train_on_all_other_hashtags(model_vars, trainer_vars, training_hashtag_names,
                                                          '', n_epochs=N_TRAIN_EPOCHS)
    print 'Mean training accuracy: %s' % training_accuracy
    print
    for hashtag_name in testing_hashtag_names:
        accuracy, _ = predict_on_hashtag(sess,
                                         model_vars,
                                         hashtag_name,
                                         HUMOR_TRIAL_TWEET_PAIR_EMBEDDING_DIR,
                                         error_analysis_stats=[SEMEVAL_HUMOR_TRIAL_DIR, 10])
        print 'Hashtag %s accuracy: %s' % (hashtag_name, accuracy)
        accuracies.append(accuracy)

    print 'Final test accuracy: %s' % np.mean(accuracies)


def build_embedding_humor_model():
    """Takes in two tweets. For each word in each tweet, the model is given a GloVe embedding and a
    phonetic embedding(generated by phoneme_model). It reads both tweets separately using an LSTM (shared vars),
    and then makes a prediction of which tweet is funnier using three fully-connected layers."""
    print 'Building embedding humor model'
    tf_batch_size = tf.placeholder(tf.int32, name='batch_size')
    word_embedding_size = GLOVE_SIZE + PHONETIC_EMB_SIZE

    tf_first_input_tweets = tf.placeholder(dtype=tf.float32, shape=[None, HUMOR_MAX_WORDS_IN_TWEET * word_embedding_size], name='first_tweets')
    tf_second_input_tweets = tf.placeholder(dtype=tf.float32, shape=[None, HUMOR_MAX_WORDS_IN_TWEET * word_embedding_size], name='second_tweets')
    tf_hashtag = tf.placeholder(dtype=tf.float32, shape=[None, HUMOR_MAX_WORDS_IN_HASHTAG * word_embedding_size], name='hashtag')

    tf_hashtag_output, tf_hashtag_hidden_state = build_lstm(word_embedding_size, tf_batch_size,
                                                            [tf_hashtag], word_embedding_size,
                                                            HUMOR_MAX_WORDS_IN_HASHTAG, lstm_scope='HASHTAG_ENCODER')

    tf_first_tweet_encoder_output, tf_first_tweet_hidden_state = build_lstm(word_embedding_size * 2, tf_batch_size,
                                                                            [tf_first_input_tweets], word_embedding_size,
                                                                            HUMOR_MAX_WORDS_IN_TWEET, lstm_scope='TWEET_ENCODER',
                                                                            time_step_inputs=[])

    tf_second_tweet_encoder_output, tf_second_tweet_hidden_state = build_lstm(word_embedding_size * 2, tf_batch_size,
                                                                              [tf_second_input_tweets], word_embedding_size,
                                                                              HUMOR_MAX_WORDS_IN_TWEET, lstm_scope='TWEET_ENCODER',
                                                                              reuse=True, time_step_inputs=[])

    # Insert character model here
    # model, tweet1_conv_emb, tweet2_conv_emb, tweet1, tweet2 = create_character_model(TWEET_SIZE)

    tf_tweet_pair_emb = tf.concat(1, [tf_first_tweet_encoder_output, tf_second_tweet_encoder_output])

    tweet_pair_emb_size = int(tf_tweet_pair_emb.get_shape()[1])

    tf_tweet_dense_layer1, _, _ = create_dense_layer(tf_tweet_pair_emb, tweet_pair_emb_size, tweet_pair_emb_size * 3 / 4, activation='relu', name='layer_1')
    tf_tweet_dense_layer1_dropout = tf.nn.dropout(tf_tweet_dense_layer1, keep_prob=DROPOUT)
    tf_tweet_dense_layer2, _, _ = create_dense_layer(tf_tweet_dense_layer1_dropout, tweet_pair_emb_size * 3 / 4, tweet_pair_emb_size / 2, activation='relu', name='layer_2')
    tf_tweet_humor_rating, _, _ = create_dense_layer(tf_tweet_dense_layer2, tweet_pair_emb_size / 2, 1, name='layer_3')

    # Code from Alexey to transform fractional predictions into prediction labels
    output_logits = tf.reshape(tf_tweet_humor_rating, [-1])
    output_prob = tf.nn.sigmoid(output_logits)
    output = tf.select(tf.greater_equal(output_prob, 0.5), tf.ones_like(output_prob, dtype=tf.int32), tf.zeros_like(output_prob, dtype=tf.int32))

    # Print model variables
    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)
    for var in trainable_vars:
        print var.name

    return [tf_first_input_tweets, tf_second_input_tweets, output, tf_tweet_humor_rating, tf_batch_size, tf_hashtag, output_prob]  # Model vars


def build_lstm(lstm_hidden_dim, tf_batch_size, inputs, input_time_step_size, num_time_steps, lstm_scope=None, reuse=False, time_step_inputs=[]):
    """Runs an LSTM over input data and returns LSTM output and hidden state. Arguments:
    lstm_hidden_dim - Size of hidden state of LSTM
    tf_batch_size - Tensor value representing size of current batch. Required for LSTM package
    inputs - Full input into LSTM. List of tensors as input. Per tensor: First dimension of m examples, with second dimension holding concatenated input for all timesteps
    input_time_step_size - Size of input from tf_input that will go into LSTM in a single timestep
    num_time_steps - Number of time steps to run LSTM
    lstm_scope - Can be a string or a scope object. Used to disambiguate variable scopes of different LSTM objects
    time_step_inputs - Inputs that are per time step. The same tensor is inserted into the model at each time step"""
    lstm = tf.nn.rnn_cell.LSTMCell(num_units=lstm_hidden_dim, state_is_tuple=True)
    tf_hidden_state = lstm.zero_state(tf_batch_size, tf.float32)
    for i in range(num_time_steps):
        # Grab time step input for each input tensor
        current_time_step_inputs = []
        for tf_input in inputs:
            current_time_step_inputs.append(tf.slice(tf_input, [0, i * input_time_step_size], [-1, input_time_step_size]))

        tf_input_time_step = tf.concat(1, current_time_step_inputs + time_step_inputs)

        with tf.variable_scope(lstm_scope) as scope:
            if i > 0 or reuse:
                scope.reuse_variables()
            tf_lstm_output, tf_hidden_state = lstm(tf_input_time_step, tf_hidden_state)
    return tf_lstm_output, tf_hidden_state


def build_embedding_humor_model_trainer(model_vars):
    """Take fractional predictions from embedding humor model. Each prediction is a number representing which
    tweet in a pair is funnier(number > 0 => first tweet funnier, number < 0 => second tweet funnier). Runs
    prediction through sigmoid to produce value between 0 and 1. Compares fractional prediction with
    actual label using tf.sigmoid_cross_entropy_with_logits() (sigmoid is done internally)."""
    print 'Building embedding humor model trainer'
    tf_labels = tf.placeholder(dtype=tf.int32, shape=[None], name='labels')
    [tf_first_input_tweets, tf_second_input_tweets, output, tf_tweet_humor_rating, tf_batch_size, tf_hashtag,
     output_prob] = model_vars
    tf_reshaped_labels = tf.cast(tf.reshape(tf_labels, [-1, 1]), tf.float32)

    tf_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(tf_tweet_humor_rating, tf_reshaped_labels)

    tf_loss = tf.reduce_sum(tf_cross_entropy) / tf.cast(tf_batch_size, tf.float32)

    return [tf_loss, tf_labels]


def train_on_all_other_hashtags(model_vars, trainer_vars, hashtag_names, hashtag_name, n_epochs=1, batch_size=50):
    """Trains on all hashtags in the SEMEVAL_HUMOR_TRAIN_DIR directory. Extracts inputs and labels from
    each hashtag, and trains in batches. Inserts input into model and evaluates output using model_vars.
    Minimizes loss defined in trainer_vars. Repeats for n_epoch epochs. For zero epochs, the model is not trained
    and an accuracy of -1 is returned."""
    if not os.path.exists(EMBEDDING_HUMOR_MODEL_DIR):
        os.makedirs(EMBEDDING_HUMOR_MODEL_DIR)
    [tf_first_input_tweets, tf_second_input_tweets, tf_predictions, tf_tweet_humor_rating, tf_batch_size, tf_hashtag, output_prob] = model_vars
    [tf_loss, tf_labels] = trainer_vars

    sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=GPU_OPTIONS))
    train_op = tf.train.AdamOptimizer(EMBEDDING_HUMOR_MODEL_LEARNING_RATE).minimize(tf_loss)
    init = tf.initialize_all_variables()
    with tf.name_scope("SAVER"):
        saver = tf.train.Saver(max_to_keep=10)
    sess.run(init)

    accuracies = []
    for epoch in range(n_epochs):
        if n_epochs > 1:
            print 'Epoch %s' % epoch
        for trainer_hashtag_name in hashtag_names:
            if trainer_hashtag_name != hashtag_name:
                # Train on this hashtag.
                np_first_tweets, np_second_tweets, np_labels, first_tweet_ids, second_tweet_ids, np_hashtag = \
                    load_hashtag_data(HUMOR_TRAIN_TWEET_PAIR_EMBEDDING_DIR, trainer_hashtag_name)
                current_batch_size = batch_size
                # Use these parameters to keep track of batch size and start location.
                starting_training_example = 0
                num_batches = np_first_tweets.shape[0] / batch_size
                remaining_batch_size = np_first_tweets.shape[0] % batch_size
                batch_accuracies = []
                batch_losses = []
                # print 'Training on hashtag %s' % trainer_hashtag_name
                for i in range(num_batches + 1):
                    # If we are on the last batch, we are running leftover examples.
                    # Otherwise, we stick to global batch_size.
                    if i == num_batches:
                        current_batch_size = remaining_batch_size
                    else:
                        current_batch_size = batch_size

                    np_batch_first_tweets = np_first_tweets[starting_training_example:starting_training_example+current_batch_size, :]
                    np_batch_second_tweets = np_second_tweets[starting_training_example:starting_training_example+current_batch_size, :]
                    np_batch_labels = np_labels[starting_training_example:starting_training_example+current_batch_size]
                    np_batch_hashtag = np_hashtag[starting_training_example:starting_training_example+current_batch_size, :]
                    # Run train step here.
                    [np_batch_predictions, batch_loss, _] = sess.run([tf_predictions, tf_loss, train_op],
                                                                     feed_dict={tf_first_input_tweets:np_batch_first_tweets,
                                                                                tf_second_input_tweets:np_batch_second_tweets,
                                                                                tf_labels: np_batch_labels,
                                                                                tf_batch_size: current_batch_size,
                                                                                tf_hashtag: np_batch_hashtag})

                    if current_batch_size > 0:
                        batch_accuracy = sklearn.metrics.accuracy_score(np_batch_labels, np_batch_predictions)

                        batch_accuracies.append(batch_accuracy)
                        batch_losses.append(batch_loss)
                        # print 'Batch accuracy: %s' % batch_accuracy
                        # print 'Batch loss: %s' % batch_loss

                    starting_training_example += current_batch_size

                hashtag_accuracy = np.mean(batch_accuracies)
                hashtag_loss = np.mean(batch_losses)
                print 'Hashtag %s accuracy: %s' % (trainer_hashtag_name, hashtag_accuracy)
                print 'Hashtag loss: %s' % hashtag_loss
                accuracies.append(hashtag_accuracy)

            else:
                print 'Do not train on current hashtag: %s' % trainer_hashtag_name
        print 'Saving..'
        saver.save(sess, os.path.join(EMBEDDING_HUMOR_MODEL_DIR, 'emb_humor_model'), global_step=epoch)  # Save model after every epoch
    if len(accuracies) > 0:
        training_accuracy = np.mean(accuracies)
    else:
        training_accuracy = -1

    # Save trained model.

    return sess, training_accuracy


def calculate_accuracy_on_batches(batch_predictions, np_labels):
    """batch_predictions is a list of numpy arrays. Each numpy
    array represents the predictions for a single batch. Evaluates
    performance of each batch using np_labels. There must be
    at least one batch containing at least one example"""
    accuracy_sum = 0
    total_examples = 0
    for np_batch_predictions in batch_predictions:
        batch_accuracy = np.mean(np_batch_predictions == np_labels)
        batch_size = np_batch_predictions.shape[0]
        if batch_size > 0:
            accuracy_sum += batch_accuracy * batch_size
            total_examples += batch_size
    return accuracy_sum / total_examples


def create_character_model(tweet_size, vocab_size):
    '''Load two tweets, analyze them with convolution and predict which is funnier.'''
    print 'Building model'
    # Model parameters I can mess with:
    num_filters_1 = 32
    num_filters_2 = 64
    filter_size_1 = 3
    filter_size_2 = 5
    dropout = 0.7
    fc1_dim = 200
    fc2_dim = 50
    tweet_emb_dim = 50
    pool_length = 5

    print 'Vocabulary size: %s' % vocab_size
    # Two tweets as input. Run them through an embedding layer
    tweet1 = Input(shape=[tweet_size])
    tweet2 = Input(shape=[tweet_size])

    tweet_input_emb_lookup = Embedding(vocab_size, tweet_emb_dim, input_length=tweet_size)
    tweet1_emb = tweet_input_emb_lookup(tweet1)
    tweet2_emb = tweet_input_emb_lookup(tweet2)

    # Run both tweets separately through convolution layers, a max pool layer,
    # and then flatten them for dense layer.
    convolution_layer_1 = Convolution1D(num_filters_1, filter_size_1, input_shape=[tweet_size, vocab_size])
    convolution_layer_2 = Convolution1D(num_filters_2, filter_size_2)
    max_pool_layer = MaxPooling1D(stride=4)
    flatten = Flatten()
    tweet_conv_emb = Dense(fc1_dim, activation='relu')

    tweet_1_conv1 = max_pool_layer(convolution_layer_1(tweet1_emb))
    tweet_2_conv1 = max_pool_layer(convolution_layer_1(tweet2_emb))
    tweet1_conv2 = flatten(max_pool_layer(convolution_layer_2(tweet_1_conv1)))
    tweet2_conv2 = flatten(max_pool_layer(convolution_layer_2(tweet_2_conv1)))
    tweet1_conv_emb = tweet_conv_emb(tweet1_conv2)
    tweet2_conv_emb = tweet_conv_emb(tweet2_conv2)

    # Combine embeddings for each tweet as inputs to two dense layers.
    tweet_pair_emb = merge([tweet1_conv_emb, tweet2_conv_emb], mode='concat')
    tweet_pair_emb = Dropout(dropout)(tweet_pair_emb)
    dense_layer1 = Dense(fc2_dim, activation='relu')(tweet_pair_emb)
    output = Dense(1, activation='sigmoid')(dense_layer1)
    model = Model(input=[tweet1, tweet2], output=[output])
    return model, tweet1_conv_emb, tweet2_conv_emb, tweet1, tweet2


if __name__ == '__main__':
    main()