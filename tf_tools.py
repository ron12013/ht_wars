"""David Donahue 2016. This script deals primarily with tensorflow build operations. This script
separates functions that do import tensorflow from those that don't."""
import tensorflow as tf
import numpy as np
import cPickle as pickle
from keras.layers import Convolution1D, MaxPooling1D
from keras.layers import Input, Dense, Flatten, Embedding
from keras.regularizers import l2
from tools import convert_words_to_indices
from tools import invert_dictionary
from tools import load_hashtag_data
from tools import extract_tweet_pair_from_hashtag_datas
from config import *
import tensorflow.contrib.slim as slim

GPU_OPTIONS = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)

MAX_WORD_SIZE = 20
MAX_PRONUNCIATION_SIZE = 20
PHONE_CHAR_EMB_DIM = 30
PHONE_ENCODER_LSTM_EMB_DIM = 200
HUMOR_DROPOUT = 1


def create_character_model(tweet_size, vocab_size):
    """Load two tweets, analyze them with convolution and predict which is funnier."""
    print 'Building model'
    # Model parameters I can mess with:
    num_filters_1 = 32
    num_filters_2 = 64
    filter_size_1 = 3
    filter_size_2 = 5
    dropout = 0.7
    fc1_dim = 200
    fc2_dim = 50
    tweet_emb_dim = 50
    pool_length = 5

    print 'Vocabulary size: %s' % vocab_size
    # Two tweets as input. Run them through an embedding layer
    tweet1 = Input(shape=[tweet_size])
    tweet2 = Input(shape=[tweet_size])

    tweet_input_emb_lookup = Embedding(vocab_size, tweet_emb_dim, input_length=tweet_size)
    tweet1_emb = tweet_input_emb_lookup(tweet1)
    tweet2_emb = tweet_input_emb_lookup(tweet2)

    # Run both tweets separately through convolution layers, a max pool layer,
    # and then flatten them for dense layer.
    convolution_layer_1 = Convolution1D(num_filters_1, filter_size_1, input_shape=[tweet_size, vocab_size])
    convolution_layer_2 = Convolution1D(num_filters_2, filter_size_2)
    max_pool_layer = MaxPooling1D(strides=4)
    flatten = Flatten()
    tweet_conv_emb = Dense(fc1_dim, activation='relu')

    tweet_1_conv1 = max_pool_layer(convolution_layer_1(tweet1_emb))
    tweet_2_conv1 = max_pool_layer(convolution_layer_1(tweet2_emb))
    tweet1_conv2 = flatten(max_pool_layer(convolution_layer_2(tweet_1_conv1)))
    tweet2_conv2 = flatten(max_pool_layer(convolution_layer_2(tweet_2_conv1)))
    tweet1_conv_emb = tweet_conv_emb(tweet1_conv2)
    tweet2_conv_emb = tweet_conv_emb(tweet2_conv2)

    return tweet1_conv_emb, tweet2_conv_emb, tweet1, tweet2


def build_humor_model(vocab_size, use_embedding_model=True, use_character_model=True, hidden_dim_size=None):
    """Takes in two tweets. For each word in each tweet, if use_embedding_model is true, the model is given a GloVe embedding and a
    phonetic embedding(generated by phoneme_model). If use_character_model is true, it
    will construct a convolutional character-based model. It groups the output of both these models by concatenation,
    and then makes a prediction of which tweet is funnier using three fully-connected layers."""
    print 'Building embedding humor model'

    tf_batch_size, tf_dropout_rate, tf_first_input_tweets, \
    tf_first_tweet_encoder_output, tf_hashtag, \
    tf_second_input_tweets, tf_second_tweet_encoder_output = create_embedding_model(lstm_hidden_dim=hidden_dim_size)

    # Create character model to feed into dense layers
    tweet1_conv_emb, tweet2_conv_emb, tf_tweet1, tf_tweet2 = create_character_model(TWEET_SIZE, vocab_size)

    # Concatenate encoders and process with dense layers
    dense_features = []
    if use_embedding_model:
        dense_features.append(tf_first_tweet_encoder_output)
        dense_features.append(tf_second_tweet_encoder_output)
    if use_character_model:
        dense_features.append(tweet1_conv_emb)
        dense_features.append(tweet2_conv_emb)
    tf_tweet_pair_emb = tf.concat(dense_features, 1)
    print tf_tweet_pair_emb.get_shape()
    tweet_pair_emb_size = int(tf_tweet_pair_emb.get_shape()[1])
    tf_tweet_pair_emb_dropout = tf.nn.dropout(tf_tweet_pair_emb, keep_prob=tf_dropout_rate)

    tf_tweet_dense_layer1, _, _ = create_dense_layer(tf_tweet_pair_emb_dropout, tweet_pair_emb_size,
                                                     tweet_pair_emb_size * 3 / 4, activation='relu', name='layer_1')
    tf_tweet_dense_layer2, _, _ = create_dense_layer(tf_tweet_dense_layer1, tweet_pair_emb_size * 3 / 4,
                                                     tweet_pair_emb_size / 2, activation='relu', name='layer_2')
    tf_tweet_humor_rating, _, _ = create_dense_layer(tf_tweet_dense_layer2, tweet_pair_emb_size / 2, 1, name='layer_3')

    # Code from Alexey to transform fractional predictions into prediction labels
    output_logits = tf.reshape(tf_tweet_humor_rating, [-1])
    output_prob = tf.nn.sigmoid(output_logits)
    output = tf.where(tf.greater_equal(output_prob, 0.5),
                      x=tf.ones_like(output_prob, dtype=tf.int32),
                      y=tf.zeros_like(output_prob, dtype=tf.int32))

    # Print model variables (debug)
    trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)
    for var in trainable_vars:
        print var.name + ' ' + str(var.get_shape())

    return [tf_first_input_tweets, tf_second_input_tweets, output, tf_tweet_humor_rating, tf_batch_size, tf_hashtag,
            output_prob, tf_dropout_rate, tf_tweet1, tf_tweet2]  # Model vars


def create_embedding_model(lstm_hidden_dim=None):
    """Applies dropout to and feeds two tweets in separate tweet encoders(shared weights)."""
    # Create placeholders
    tf_batch_size = tf.placeholder(tf.int32, name='batch_size')
    tf_dropout_rate = tf.placeholder(tf.float32, name='dropout_rate')
    word_embedding_size = GLOVE_EMB_SIZE + PHONETIC_EMB_SIZE
    if lstm_hidden_dim is None:
        lstm_hidden_dim = word_embedding_size * 2
    tf_first_input_tweets = tf.placeholder(dtype=tf.float32,
                                           shape=[None, HUMOR_MAX_WORDS_IN_TWEET * word_embedding_size],
                                           name='first_tweets')
    tf_second_input_tweets = tf.placeholder(dtype=tf.float32,
                                            shape=[None, HUMOR_MAX_WORDS_IN_TWEET * word_embedding_size],
                                            name='second_tweets')
    tf_hashtag = tf.placeholder(dtype=tf.float32, shape=[None, HUMOR_MAX_WORDS_IN_HASHTAG * word_embedding_size],
                                name='hashtag')
    # Create tweet LSTM encoders to feed into dense layers
    tf_first_input_tweets_dropout = tf.nn.dropout(tf_first_input_tweets, tf_dropout_rate)
    tf_second_input_tweets_dropout = tf.nn.dropout(tf_second_input_tweets, tf_dropout_rate)

    tf_first_tweet_encoder_output, tf_first_tweet_hidden_state = build_lstm(lstm_hidden_dim, tf_batch_size,
                                                                            [tf_first_input_tweets_dropout],
                                                                            word_embedding_size,
                                                                            HUMOR_MAX_WORDS_IN_TWEET,
                                                                            lstm_scope='TWEET_ENCODER',
                                                                            time_step_inputs=[])
    tf_second_tweet_encoder_output, tf_second_tweet_hidden_state = build_lstm(lstm_hidden_dim, tf_batch_size,
                                                                              [tf_second_input_tweets_dropout],
                                                                              word_embedding_size,
                                                                              HUMOR_MAX_WORDS_IN_TWEET,
                                                                              lstm_scope='TWEET_ENCODER',
                                                                              reuse=True, time_step_inputs=[])
    return tf_batch_size, tf_dropout_rate, tf_first_input_tweets, tf_first_tweet_encoder_output, tf_hashtag, tf_second_input_tweets, tf_second_tweet_encoder_output


def build_lstm(lstm_hidden_dim, tf_batch_size, inputs, input_time_step_size, num_time_steps, lstm_scope=None,
               reuse=False, time_step_inputs=None):
    """Runs an LSTM over input data and returns LSTM output and hidden state. Arguments:
    lstm_hidden_dim - Size of hidden state of LSTM
    tf_batch_size - Tensor value representing size of current batch. Required for LSTM package
    inputs - Full input into LSTM. List of tensors as input. Per tensor: First dimension of m examples, with second dimension holding concatenated input for all timesteps
    input_time_step_size - Size of input from tf_input that will go into LSTM in a single timestep
    num_time_steps - Number of time steps to run LSTM
    lstm_scope - Can be a string or a scope object. Used to disambiguate variable scopes of different LSTM objects
    time_step_inputs - Inputs that are per time step. The same tensor is inserted into the model at each time step"""
    if time_step_inputs is None:
        time_step_inputs = []
    lstm = tf.contrib.rnn.LSTMCell(num_units=lstm_hidden_dim, state_is_tuple=True)
    tf_hidden_state = lstm.zero_state(tf_batch_size, tf.float32)
    for i in range(num_time_steps):
        # Grab time step input for each input tensor
        current_time_step_inputs = []
        for tf_input in inputs:
            current_time_step_inputs.append(
                tf.slice(tf_input, [0, i * input_time_step_size], [-1, input_time_step_size]))

        tf_input_time_step = tf.concat(current_time_step_inputs + time_step_inputs, 1)

        with tf.variable_scope(lstm_scope) as scope:
            if i > 0 or reuse:
                scope.reuse_variables()
            tf_lstm_output, tf_hidden_state = lstm(tf_input_time_step, tf_hidden_state)
    return tf_lstm_output, tf_hidden_state


def build_chars_to_phonemes_model(char_vocab_size, phone_vocab_size):
    """Here we build a model that takes in a series of characters and outputs a series of phonemes.
    The model, once trained, can pronounce words."""
    print 'Building model'
    with tf.name_scope('CHAR_TO_PHONE_MODEL'):
        # PLACEHOLDERS. Model takes in a sequence of characters contained in tf_words.
        # The model also needs to know the batch size.
        tf_batch_size = tf.placeholder(tf.int32, name='batch_size')
        tf_words = tf.placeholder(tf.int32, [None, MAX_WORD_SIZE], 'words')
        # Lookup up embeddings for all characters in each word.
        tf_char_emb = tf.Variable(tf.random_normal([char_vocab_size, PHONE_CHAR_EMB_DIM]), name='character_emb')
        # Insert each character one by one into an LSTM.
        lstm = tf.contrib.rnn.LSTMCell(num_units=PHONE_ENCODER_LSTM_EMB_DIM, state_is_tuple=True)
        encoder_hidden_state = lstm.zero_state(tf_batch_size, tf.float32)
        for i in range(MAX_WORD_SIZE):
            tf_char_embedding = tf.nn.embedding_lookup(tf_char_emb, tf_words[:, i])

            with tf.variable_scope('LSTM_ENCODER') as lstm_scope:
                if i > 0:
                    lstm_scope.reuse_variables()
                encoder_output, encoder_hidden_state = lstm(tf_char_embedding, encoder_hidden_state)
        # Run encoder output through dense layer to process output
        tf_encoder_output_w = tf.Variable(tf.random_normal([PHONE_ENCODER_LSTM_EMB_DIM, PHONE_ENCODER_LSTM_EMB_DIM]), name='encoder_output_emb')
        tf_encoder_output_b = tf.Variable(tf.random_normal([PHONE_ENCODER_LSTM_EMB_DIM]), name='encoder_output_bias')
        encoder_output_emb = tf.matmul(encoder_output, tf_encoder_output_w) + tf_encoder_output_b

        decoder_hidden_state = lstm.zero_state(tf_batch_size, tf.float32)

        # Use hidden state of character encoding stage (this is the phoneme embedding) to predict phonemes.
        phonemes = []
        tf_phone_pred_w = tf.Variable(tf.random_normal([lstm.output_size, phone_vocab_size]),
                                      name='phoneme_prediction_emb')
        tf_phone_pred_b = tf.Variable(tf.random_normal([phone_vocab_size]), name='phoneme_prediction_bias')
        for j in range(MAX_PRONUNCIATION_SIZE):
            with tf.variable_scope('LSTM_DECODER') as lstm_scope:
                if j == 0:
                    decoder_output, decoder_hidden_state = lstm(encoder_output_emb, decoder_hidden_state)
                else:
                    lstm_scope.reuse_variables()
                    # decoder_output, decoder_hidden_state = lstm(tf.zeros([tf_batch_size, LSTM_EMB_DIM]), decoder_hidden_state)
                    decoder_output, decoder_hidden_state = lstm(encoder_output_emb, decoder_hidden_state)
                phoneme = tf.matmul(decoder_output, tf_phone_pred_w) + tf_phone_pred_b
                phonemes.append(phoneme)
        tf_phonemes = tf.stack(phonemes, axis=1)
    # Print model variables.
    model_variables = tf.trainable_variables()
    print 'Model variables:'
    # for model_variable in model_variables:
    #     print ' - ', model_variable.name

    return [tf_words, tf_batch_size], [tf_phonemes, encoder_output_emb]


def create_dense_layer(input_layer, input_size, output_size, activation=None, include_bias=True, reg_const=.0005, name=None):
    with tf.name_scope(name):
        tf_w = tf.Variable(tf.random_normal([input_size, output_size], stddev=.1))
        tf_b = tf.Variable(tf.random_normal([output_size]))
        output_layer = tf.matmul(input_layer, tf_w)
        if include_bias:
            output_layer = output_layer + tf_b
        if activation == 'relu':
            output_layer = tf.nn.relu(output_layer)
        elif activation == 'sigmoid':
            output_layer = tf.nn.sigmoid(output_layer)
        elif activation is None:
            pass
        else:
            print 'Error: Did not specify layer activation'

    regularizer = slim.l2_regularizer(reg_const)
    regularizer_loss = regularizer(tf_w) + regularizer(tf_b)
    slim.losses.add_loss(regularizer_loss)

    return output_layer, tf_w, tf_b


def generate_phonetic_embs_from_words(words, char_to_index_path, phone_to_index_path):
    """Generates a phonetic embedding for each word using the pretrained char2phone model."""
    print 'Generating phonetic embeddings for GloVe words'
    char_to_index = pickle.load(open(char_to_index_path, 'rb'))
    phone_to_index = pickle.load(open(phone_to_index_path, 'rb'))
    character_vocab_size = len(char_to_index)
    phoneme_vocab_size = len(phone_to_index)
    model_inputs, model_outputs = build_chars_to_phonemes_model(character_vocab_size, phoneme_vocab_size)
    [tf_words, tf_batch_size] = model_inputs
    [tf_phonemes, lstm_hidden_state] = model_outputs
    tf_phonetic_emb = lstm_hidden_state

    np_word_indices = convert_words_to_indices(words, char_to_index)
    print np_word_indices
    # Prove words converted to indices correctly by reversing the process and printing.
    index_to_char = invert_dictionary(char_to_index)
    print 'Example GloVe words recreated from indices:'
    for i in range(130, 140):
        np_word = np_word_indices[i, :]
        char_list = []
        for j in np_word:
            if j in index_to_char:
                char_list.append(index_to_char[j])
        word = ''.join(char_list)
        print word,
    print

    sess = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=GPU_OPTIONS))
    saver = tf.train.Saver(max_to_keep=10)
    # Restore model from previous save.
    ckpt = tf.train.get_checkpoint_state(CHAR_2_PHONE_MODEL_DIR)
    if ckpt and ckpt.model_checkpoint_path:
        saver.restore(sess, ckpt.model_checkpoint_path)
    else:
        print("No checkpoint found!")
        return -1

    np_phonetic_emb = sess.run(tf_phonetic_emb, feed_dict={tf_words: np_word_indices,
                                                           tf_batch_size: len(words)})

    print np_phonetic_emb.shape
    print np.mean(np.abs(np_phonetic_emb))

    return np_phonetic_emb


def create_tensorboard_visualization(model_name):
    """Saves the Tensorflow graph of your model, so you can view it in a TensorBoard console."""
    print 'Creating Tensorboard visualization'
    writer = tf.summary.FileWriter("/tmp/" + model_name + "/")
    writer.add_graph(tf.get_default_graph())


def predict_on_hashtag(sess, model_vars, hashtag_name, hashtag_dir, hashtag_datas, error_analysis_stats=None):
    """Predicts on a hashtag. Returns the accuracy of predictions on all tweet pairs and returns
    a list. The list contains the predictions on all tweet pairs, and tweet ids for the first and second tweets in
    each pair. If error analysis stats are provided, the function will print the tweet pairs the model performed the worst
    on. error_analysis_stats should be a string and a number. The string is the location where Semeval hashtag .tsv files are kept(training or testing).
    The number is the number of worst tweet pairs to print."""
    print 'Predicting on hashtag %s' % hashtag_name
    np_first_tweets_char, np_second_tweets_char = extract_tweet_pair_from_hashtag_datas(hashtag_datas, hashtag_name)

    [tf_first_input_tweets, tf_second_input_tweets, tf_predictions, tf_tweet_humor_rating, tf_batch_size, tf_hashtag, tf_output_prob, tf_dropout_rate,
     tf_tweet1, tf_tweet2] = model_vars

    np_first_tweets, np_second_tweets, np_labels, first_tweet_ids, second_tweet_ids, np_hashtag = load_hashtag_data(hashtag_dir, hashtag_name)
    np_predictions, np_output_prob = sess.run([tf_predictions, tf_output_prob],
                                              feed_dict={tf_first_input_tweets: np_first_tweets,
                                                         tf_second_input_tweets: np_second_tweets,
                                                         tf_batch_size: np_first_tweets.shape[0],
                                                         tf_hashtag: np_hashtag,
                                                         tf_dropout_rate: 1.0,
                                                         tf_tweet1: np_first_tweets_char,
                                                         tf_tweet2: np_second_tweets_char})

    accuracy = None
    if np_labels is not None:
        accuracy = np.mean(np_predictions == np_labels)
    return accuracy, [np_predictions, first_tweet_ids, second_tweet_ids]
